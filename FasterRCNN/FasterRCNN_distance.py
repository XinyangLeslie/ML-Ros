#!/usr/bin/env python
# coding: utf-8

# ## åˆ’åˆ†æ•°æ®é›†

import os
import random
import shutil

# è®¾ç½®è·¯å¾„
base_dir = os.getcwd()
image_dir = os.path.join(base_dir, 'images_new')
label_dir = os.path.join(base_dir, 'label_distance')

# ç›®æ ‡æ–‡ä»¶å¤¹
train_img_dir = os.path.join(base_dir, 'train/images')
train_lbl_dir = os.path.join(base_dir, 'train/labels')
val_img_dir = os.path.join(base_dir, 'val/images')
val_lbl_dir = os.path.join(base_dir, 'val/labels')

# åˆ›å»ºæ–‡ä»¶å¤¹
for folder in [train_img_dir, train_lbl_dir, val_img_dir, val_lbl_dir]:
    os.makedirs(folder, exist_ok=True)

# è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶åï¼ˆç¡®ä¿æŒ‰ jpg åç¼€ï¼‰
all_images = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]
random.shuffle(all_images)

# 8:2 åˆ’åˆ†
split_idx = int(len(all_images) * 0.8)
train_files = all_images[:split_idx]
val_files = all_images[split_idx:]

def copy_files(file_list, target_img_dir, target_lbl_dir):
    for img_file in file_list:
        label_file = img_file.replace('.jpg', '.txt')
        src_img = os.path.join(image_dir, img_file)
        src_lbl = os.path.join(label_dir, label_file)
        dst_img = os.path.join(target_img_dir, img_file)
        dst_lbl = os.path.join(target_lbl_dir, label_file)

        shutil.copy2(src_img, dst_img)
        if os.path.exists(src_lbl):
            shutil.copy2(src_lbl, dst_lbl)

# æ‹·è´
copy_files(train_files, train_img_dir, train_lbl_dir)
copy_files(val_files, val_img_dir, val_lbl_dir)

print(f"âœ… æ•°æ®åˆ’åˆ†å®Œæˆï¼šè®­ç»ƒé›† {len(train_files)} å¼ ï¼ŒéªŒè¯é›† {len(val_files)} å¼ ")


# ## æ£€æŸ¥æ•°æ®é›†

# In[1]:


import os
import cv2
import matplotlib.pyplot as plt

# é…ç½®è·¯å¾„
image_dir = './train/images'
label_dir = './train/labels'

id_to_name = {
    0: 'suitcase',
    1: 'person',
    2: 'table',
    3: 'chair'
}

# é€‰æ‹©ä¸€å¼ å›¾
idx = 19
image_files = sorted(os.listdir(image_dir))
img_name = image_files[idx]
img_path = os.path.join(image_dir, img_name)
label_path = os.path.join(label_dir, img_name.replace('.jpg', '.txt'))

# è¯»å–å›¾ç‰‡
image = cv2.imread(img_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
h, w, _ = image.shape

# è¯»å–æ ‡ç­¾å¹¶ç»˜å›¾
if os.path.exists(label_path):
    with open(label_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) < 6:
                continue
            cls_id, cx, cy, bw, bh, dist = parts
            cls_id = int(float(cls_id))

            label = id_to_name.get(cls_id, f"class {cls_id}")
            cx, cy, bw, bh = float(cx)*w, float(cy)*h, float(bw)*w, float(bh)*h
            x1 = int(cx - bw/2)
            y1 = int(cy - bh/2)
            x2 = int(cx + bw/2)
            y2 = int(cy + bh/2)

            # ç»˜åˆ¶çŸ©å½¢æ¡†
            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # ç±»åˆ« + è·ç¦»æ ‡ç­¾
            label_text = f"{label} | {float(dist):.2f}m"
            cv2.putText(image, label_text, (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

# æ˜¾ç¤ºå›¾åƒ
plt.figure(figsize=(10, 6))
plt.imshow(image)
plt.title(f"Image: {img_name}")
plt.axis('off')
plt.show()


# In[3]:


import os
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import torchvision.transforms.functional as F
import torchvision.transforms as T
import torchvision
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm import tqdm
import random
import matplotlib.pyplot as plt
from torchvision.ops import box_iou

# âœ… Dataset ç±»
class YoloToFRCNNDataset(Dataset):
    def __init__(self, image_dir, label_dir, train=True):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.image_files = sorted(os.listdir(image_dir))
        self.train = train

        # ç±»åˆ«æ˜ å°„ï¼šYOLOåŸå§‹ID => è¿ç»­ç´¢å¼•
        self.id_to_name = {
            0: 'suitcase',
            1: 'person',
            2: 'table',
            3: 'chair'
        }

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.image_dir, img_name)
        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt'))

        image = Image.open(img_path).convert("RGB")
        w, h = image.size
        boxes = []
        labels = []
        distances = []

        if os.path.exists(label_path):
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) < 6:
                        continue
                    cls_id, cx, cy, bw, bh, dist = parts
                    cls_id = int(float(cls_id))  # è½¬ä¸ºæ•´æ•°

                    label = cls_id

                    # åå½’ä¸€åŒ–
                    cx, cy, bw, bh = float(cx) * w, float(cy) * h, float(bw) * w, float(bh) * h
                    x1 = cx - bw / 2
                    y1 = cy - bh / 2
                    x2 = cx + bw / 2
                    y2 = cy + bh / 2

                    boxes.append([x1, y1, x2, y2])
                    labels.append(label)
                    distances.append(float(dist))

        # æ•°æ®å¢å¼º
        if self.train:
            if random.random() < 0.5:
                image = F.hflip(image)
                for i in range(len(boxes)):
                    x1, y1, x2, y2 = boxes[i]
                    boxes[i] = [w - x2, y1, w - x1, y2]
            image = F.adjust_brightness(image, 1 + (random.random() - 0.5) * 0.4)
            image = F.adjust_contrast(image, 1 + (random.random() - 0.5) * 0.4)

        image_tensor = T.ToTensor()(image)

        target = {
            "boxes": torch.tensor(boxes, dtype=torch.float32),
            "labels": torch.tensor(labels, dtype=torch.int64),
            "image_id": torch.tensor([idx]),
            "distances": torch.tensor(distances, dtype=torch.float32),
        }

        return image_tensor, target


# In[5]:


import torch
import torch.nn as nn
from torchvision.models.detection import FasterRCNN

class FasterRCNNWithDistance(FasterRCNN):
    def __init__(self, backbone, num_classes):
        super().__init__(backbone, num_classes)
        in_features = self.roi_heads.box_predictor.cls_score.in_features
        self.distance_head = nn.Sequential(
            nn.Linear(in_features, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

    def forward(self, images, targets=None):
        if self.training:
            # ğŸ”§ åŸå§‹æŸå¤±
            loss_dict = super().forward(images, targets)

            # ğŸ”§ ç‰¹å¾æå–
            features = self.backbone(torch.stack(images))  # OrderedDict[str, Tensor]

            # ğŸ”§ æ‰€æœ‰ GT æ¡†
            all_gt_boxes = [t["boxes"] for t in targets]
            image_shapes = [img.shape[1:] for img in images]

            # ğŸ”§ RoI ç‰¹å¾æå–
            box_features = self.roi_heads.box_roi_pool(features, all_gt_boxes, image_shapes)
            box_features = self.roi_heads.box_head(box_features)

            # ğŸ”§ è·ç¦»é¢„æµ‹ + loss
            pred_distances = self.distance_head(box_features).squeeze(1)
            gt_distances = torch.cat([t["distances"] for t in targets]).to(pred_distances.device)
            distance_loss = nn.functional.smooth_l1_loss(pred_distances, gt_distances)

            loss_dict["loss_distance"] = distance_loss
            return loss_dict

        else:
            # ğŸ§Š æ¨ç†æ¨¡å¼
            detections = super().forward(images)

            # ğŸ”§ æå– featuresï¼ˆOrderedDict[str, Tensor]ï¼‰
            features = self.backbone(torch.stack(images))

            all_boxes = [d["boxes"] for d in detections]
            image_shapes = [img.shape[1:] for img in images]

            if sum(len(b) for b in all_boxes) == 0:
                # æ²¡æœ‰ä»»ä½•é¢„æµ‹æ¡†ï¼Œç›´æ¥è¿”å›ç©ºè·ç¦»
                for det in detections:
                    det["distances"] = torch.tensor([]).to(images[0].device)
                return detections

            # ğŸ”§ æå– RoI ç‰¹å¾
            box_features = self.roi_heads.box_roi_pool(features, all_boxes, image_shapes)
            box_features = self.roi_heads.box_head(box_features)
            pred_distances = self.distance_head(box_features).squeeze(1)

            # ğŸ”§ å°†è·ç¦»ç»“æœæ‹†åˆ†åˆ°æ¯å¼ å›¾åƒ
            start = 0
            for i in range(len(detections)):
                num_boxes = len(detections[i]["boxes"])
                if num_boxes == 0:
                    detections[i]["distances"] = torch.tensor([]).to(images[0].device)
                else:
                    detections[i]["distances"] = pred_distances[start:start+num_boxes].detach().cpu()
                    start += num_boxes

            return detections


# In[7]:


import torch
import torchvision.transforms as T
import matplotlib.pyplot as plt
import cv2

def visualize_prediction(model, image_path, device, id_to_name, score_thresh=0.5):
    # åŠ è½½å›¾ç‰‡
    orig = cv2.imread(image_path)
    image = cv2.cvtColor(orig.copy(), cv2.COLOR_BGR2RGB)
    h, w, _ = image.shape

    # å›¾åƒé¢„å¤„ç†
    transform = T.Compose([
        T.ToTensor()
    ])
    img_tensor = transform(image).to(device)

    # æ¨¡å‹æ¨ç†
    model.eval()
    with torch.no_grad():
        outputs = model([img_tensor])[0]

    boxes = outputs['boxes'].cpu()
    labels = outputs['labels'].cpu()
    scores = outputs['scores'].cpu()
    distances = outputs['distances'].cpu() if 'distances' in outputs else None

    # ç»˜åˆ¶æ¡†
    for box, label, score, dist in zip(boxes, labels, scores, distances):
        if score < score_thresh:
            continue
        x1, y1, x2, y2 = map(int, box)
        name = id_to_name.get(label.item(), f"cls {label.item()}")
        dist_str = f"{dist:.2f}m" if distances is not None else ""
        label_str = f"{name} | {dist_str}"

        cv2.rectangle(orig, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(orig, label_str, (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

    # æ˜¾ç¤ºå›¾åƒ
    plt.figure(figsize=(10, 6))
    plt.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB))
    plt.title(f"Prediction: {image_path}")
    plt.axis('off')
    plt.show()


# In[9]:


#from model_with_distance import FasterRCNNWithDistance
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone

# ç±»åˆ«æ˜ å°„ï¼ˆä½ çš„è¿ç»­ IDï¼‰
id_to_name = {
    0: 'suitcase',
    1: 'person',
    2: 'table',
    3: 'chair'
}

# æ¨¡å‹åŠ è½½
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
backbone = resnet_fpn_backbone('resnet101', pretrained=False)
model = FasterRCNNWithDistance(backbone, num_classes=5).to(device)
model.load_state_dict(torch.load("best_model_distance.pth", map_location=device))
model.eval()

# æ¨ç†ä¸€å¼ å›¾ç‰‡
image_path = "./val/images/frame_00000_2.jpg"  # ğŸ‘ˆ æ›¿æ¢ä¸ºä½ çš„ä¸€å¼ å›¾
visualize_prediction(model, image_path, device, id_to_name)


# In[11]:


def collate_fn(batch):
    return tuple(zip(*batch))

# âœ… æ¨¡å‹æ„å»ºï¼ˆResNet101 + FPNï¼‰
def get_model(num_classes):
    backbone = resnet_fpn_backbone('resnet101', pretrained=True)
    model = FasterRCNNWithDistance(backbone, num_classes)
    return model


# âœ… éªŒè¯å‡½æ•°ï¼ˆæ¯è½®è¯„ä¼°ï¼‰
import torch
from torchvision.ops import box_iou

@torch.no_grad()
def evaluate_model(model, val_loader, device="cuda", iou_threshold=0.6, score_threshold=0.5):
    model.eval()
    model.to(device)

    total_preds = 0
    total_gts = 0
    correct = 0
    correct_total_images = 0
    distance_errors = []  # âœ… ç”¨äºè®°å½•é¢„æµ‹ä¸ GT è·ç¦»çš„å·®å¼‚

    for images, targets in val_loader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        outputs = model(images)

        for pred, target in zip(outputs, targets):
            pred_boxes = pred['boxes']
            pred_scores = pred['scores']
            pred_labels = pred['labels']
            pred_dists = pred['distances'] if 'distances' in pred else None

            gt_boxes = target['boxes']
            gt_labels = target['labels']
            gt_dists = target['distances']

            keep = pred_scores > score_threshold
            pred_boxes = pred_boxes[keep]
            pred_labels = pred_labels[keep]
            if pred_dists is not None:
                keep = keep.to(pred_dists.device)  # âœ… ä¿è¯ keep å’Œ pred_dists åœ¨åŒä¸€è®¾å¤‡
                pred_dists = pred_dists[keep]

            total_preds += len(pred_boxes)
            total_gts += len(gt_boxes)

            matched = 0
            if len(pred_boxes) > 0 and len(gt_boxes) > 0:
                ious = box_iou(pred_boxes, gt_boxes)
                for i in range(len(pred_boxes)):
                    max_iou, gt_idx = ious[i].max(0)
                    if max_iou > iou_threshold and pred_labels[i] == gt_labels[gt_idx]:
                        correct += 1
                        matched += 1
                        ious[:, gt_idx] = 0  # é˜²æ­¢é‡å¤åŒ¹é…

                        # âœ… è®¡ç®—è·ç¦»è¯¯å·®
                        if pred_dists is not None and gt_idx < len(gt_dists):
                            pred_dist = pred_dists[i].item()
                            gt_dist = gt_dists[gt_idx].item()
                            distance_errors.append(abs(pred_dist - gt_dist))

            if matched > 0:
                correct_total_images += 1

    precision = correct / total_preds if total_preds > 0 else 0
    recall = correct / total_gts if total_gts > 0 else 0
    accuracy = correct_total_images / len(val_loader.dataset)

    avg_distance_error = sum(distance_errors) / len(distance_errors) if distance_errors else -1

    return precision, recall, accuracy, avg_distance_error


# âœ… æ•°æ® & æ¨¡å‹å‡†å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
train_dataset = YoloToFRCNNDataset('./train/images', './train/labels', train=True)
val_dataset = YoloToFRCNNDataset('./val/images', './val/labels', train=False)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)
model = get_model(num_classes=5)
model.to(device)
print(torch.cuda.is_available())  # True æ‰è¯´æ˜æœ‰ GPU
print(device)                     # åº”è¯¥æ˜¯ 'cuda'


# âœ… åŠ è½½å·²ä¿å­˜çš„æƒé‡å’Œ best_loss
best_loss = float('inf')
# if os.path.exists("best_model_resnet101.pth"):
#     model.load_state_dict(torch.load("best_model_resnet101.pth", map_location=device))
#     print("âœ… Loaded best_model_resnet101.pth")

if os.path.exists("best_loss.txt"):
    with open("best_loss.txt", "r") as f:
        best_loss = float(f.read())
    print(f"ğŸ“‰ Loaded previous best loss: {best_loss:.4f}")

params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-4)
lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)

# âœ… è®­ç»ƒ + è¯„ä¼°ä¸»å¾ªç¯
num_epochs = 50
loss_list = []
precision_list = []
recall_list = []
acc_list = []

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    pbar = tqdm(train_loader, desc=f"ğŸ“¦ Epoch {epoch+1}/{num_epochs}")

    for images, targets in pbar:
        images = list(img.to(device) for img in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
    
        # âœ… è¿‡æ»¤æ‰ boxes æ•°é‡ä¸º 0 çš„æ ·æœ¬
        filtered_images = []
        filtered_targets = []
        for img, tgt in zip(images, targets):
            if tgt['boxes'].numel() > 0:
                filtered_images.append(img)
                filtered_targets.append(tgt)
    
        if len(filtered_images) == 0:
            continue  # å…¨æ˜¯ç©ºæ ‡ç­¾ï¼Œè·³è¿‡è¯¥ batch
    
        loss_dict = model(filtered_images, filtered_targets)
        #print(loss_dict)

        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()
        total_loss += losses.item()
        pbar.set_postfix({k: f"{v.item():.4f}" for k, v in loss_dict.items()})

    avg_loss = total_loss / len(train_loader)
    loss_list.append(avg_loss)
    lr_scheduler.step(avg_loss)

    # âœ… æ¯è½®è¯„ä¼°
    precision, recall, acc, dist_err = evaluate_model(model, val_loader, device=device)

    print(f"\nğŸ“Š Epoch {epoch+1}: Avg Loss={avg_loss:.4f} | ğŸ¯ Precision={precision:.4f} | ğŸ” Recall={recall:.4f} | âœ… Accuracy={acc:.4f} | ğŸ“ AvgDistError={dist_err:.2f}m\n")

    precision_list.append(precision)
    recall_list.append(recall)
    acc_list.append(acc)

    if avg_loss < best_loss:
        best_loss = avg_loss
        torch.save(model.state_dict(), "best_model_distance.pth")
        with open("best_loss.txt", "w") as f:
            f.write(str(best_loss))
        print("âœ… Saved best model.")

# âœ… è®­ç»ƒå®Œæˆï¼Œç»˜åˆ¶å¤šå›¾å¯¹æ¯”
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.plot(loss_list, marker='o')
plt.title("Loss")
plt.grid(True)

plt.subplot(1, 3, 2)
plt.plot(precision_list, marker='o', label='Precision')
plt.plot(recall_list, marker='s', label='Recall')
plt.title("Precision / Recall")
plt.legend()
plt.grid(True)

plt.subplot(1, 3, 3)
plt.plot(acc_list, marker='^', color='green')
plt.title("Accuracy (image level)")
plt.grid(True)

plt.tight_layout()
plt.show()


# In[13]:


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# In[15]:


print(device)


# In[ ]:




